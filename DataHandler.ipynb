{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeiltonmsantos/DataHandler/blob/main/DataHandler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J0q4CM6x8ju"
      },
      "source": [
        "---\n",
        "---\n",
        "># **INICIALIZAÇÕES**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xrM0MMue1FXK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title INICIALIZE A APLICAÇÃO AQUI\n",
        "!pip install pdfplumber\n",
        "!pip install pyspark\n",
        "!pip install google-generativeai\n",
        "!pip install tabulate==0.9.0\n",
        "\n",
        "# @title CARREGANDO O MANIPULADOR DE DADOS\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from pyspark import SparkContext\n",
        "import ast\n",
        "import os\n",
        "import time\n",
        "import datetime as dtm\n",
        "import tabulate as tb\n",
        "from IPython.display import clear_output\n",
        "# import tzlocal\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "clear_output()\n",
        "print('APLICAÇÃO INICIALIZADA COM SUCESSO!')\n",
        "time.sleep(3)\n",
        "clear_output()\n",
        "\n",
        "try:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  DIR_PROJECT = '/content/drive/MyDrive/Colab Notebooks/DataHandler'\n",
        "\n",
        "  # Verificando se existe a pasta do projeto. Se não existir, cria\n",
        "  if os.path.isdir(DIR_PROJECT) == False:\n",
        "    os.mkdir(DIR_PROJECT)\n",
        "    # Movendo o arquivo DataHandler.ipynb recém uploaded para a pasta do projeto\n",
        "    uploaded_file = '/content/drive/MyDrive/Colab Notebooks/DataHandler.ipynb'\n",
        "    new_file = DIR_PROJECT + '/DataHandler.ipynb'\n",
        "    os.rename(uploaded_file, new_file)\n",
        "\n",
        "  # Verificando se existe a pasta de análise de dados. Se não exisitir, cria\n",
        "  if not os.path.isdir(DIR_PROJECT + '/analysis'):\n",
        "    os.mkdir(DIR_PROJECT + '/analysis')\n",
        "except:\n",
        "  print('Alguma instabilidade na rede não permitiu acesso ao Google Drive')\n",
        "  print('Execute esta célula mais uma vez')\n",
        "  raise\n",
        "\n",
        "# Criando arquivo txt com chave de API Gemini, se não existir\n",
        "try:\n",
        "  if not os.path.exists(DIR_PROJECT + '/gemini-api-key.txt'):\n",
        "    clear_output()\n",
        "    msg = 'Você ainda não salvou a chave de API Gemini. Ela é necessária para que'\n",
        "    msg += '\\na aplicação funcione corretamente. Copie sua chave na página do Google'\n",
        "    msg += '\\nAI Studio em https://aistudio.google.com/app/u/1/apikey. Em seguida,'\n",
        "    msg += '\\ndigite ou cole logo abaixo'\n",
        "    print(tb.tabulate([[msg]], headers=['ATENÇÃO'], tablefmt='fancy_grid'))\n",
        "    print('\\n')\n",
        "    key_gemini = input('Digite ou cole aqui sua chave do API Gemini: ')\n",
        "    with open(DIR_PROJECT + '/gemini-api-key.txt', 'w') as f:\n",
        "      f.write(key_gemini)\n",
        "    msg = 'Chave de API Gemini salva com sucesso!'\n",
        "    print(tb.tabulate([[msg]], headers=['OPERAÇÃO CONLUÍDA'], tablefmt='fancy_grid'))\n",
        "    time.sleep(4)\n",
        "    clear_output()\n",
        "except KeyboardInterrupt:\n",
        "  clear_output()\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "\n",
        "# Função para imprimir mensagem, dado um título (title) e uma string para a\n",
        "# mensagem (msg). O parâmetro opcional 'delay', se informado, faz a mensagem\n",
        "# desaparecer após o valor informado de 'delay' em segundos. Se 'title' for uma\n",
        "# lista, cada valor representa o rótulo de uma coluna e 'msg' deve ser uma lista\n",
        "# de listas, cada uma dessas listas uma linha (a mensagem toda será uma tabela)\n",
        "def printMessage(msg, title, delay=0):\n",
        "  if type(msg) is list:\n",
        "    print(tb.tabulate(msg, headers=[title], tablefmt='fancy_grid'))\n",
        "  else:\n",
        "    print(tb.tabulate([[msg]], headers=[title], tablefmt='fancy_grid'))\n",
        "  if delay > 0:\n",
        "    time.sleep(delay)\n",
        "    clear_output()\n",
        "\n",
        "\n",
        "class DataHandler:\n",
        "  def __init__(self, dt=False, filename=None, coment=None):\n",
        "\n",
        "    # Carregando chave da API Gemini\n",
        "    with open(DIR_PROJECT + '/gemini-api-key.txt', 'r') as f:\n",
        "      genai.configure(api_key=f.read().strip())\n",
        "\n",
        "    # String com data/hora não informada (novos dados). Criando nova string...\n",
        "    if dt is False:\n",
        "      dt_hr = dtm.datetime.now()\n",
        "      lst_dt = str(dt_hr).split(' ')\n",
        "      self.dt = lst_dt[0] + '|' + lst_dt[1]\n",
        "      # DataFrame Pandas com todos os dados\n",
        "      self.filename = filename\n",
        "      self.full_df = self.miningDataInPDF()\n",
        "      # DataFrame Pandas com todos os reprovados\n",
        "      self.reprov_df = None\n",
        "      # DataFrame c/ reprovados por segmento\n",
        "      self.reprov_classif_df = None\n",
        "      # Salvando comentário\n",
        "      self.coment = coment\n",
        "      with open(DIR_PROJECT + '/analysis/' + self.dt + '/coment.txt', 'w') as f:\n",
        "        f.write(self.coment)\n",
        "\n",
        "    # String com data/hora foi informado. Carregando dados salvos em CSVs...\n",
        "    else:\n",
        "      self.dt = dt\n",
        "      # DataFrame Pandas com todos os dados\n",
        "      try:\n",
        "        self.full_df = pd.read_csv(DIR_PROJECT + '/analysis/' + self.dt + '/full_data.csv')\n",
        "      except Exception:\n",
        "        self.full_df = None\n",
        "      # DataFrame Pandas com todos os reprovados\n",
        "      try:\n",
        "        self.reprov_df = pd.read_csv(DIR_PROJECT + '/analysis/' + self.dt + '/reprov_data.csv')\n",
        "      except Exception:\n",
        "        self.reprov_df = None\n",
        "      # DataFrame c/ reprovados por segmento\n",
        "      try:\n",
        "        self.reprov_classif_df = pd.read_csv(DIR_PROJECT + '/analysis/' + self.dt + '/classif_reprov_data.csv')\n",
        "      except Exception:\n",
        "        self.reprov_classif_df = None\n",
        "\n",
        "      self.filename = None\n",
        "\n",
        "    # Tamamanho do bloco de dados analisados de uma vez c/ IA p/ obter segmentos\n",
        "    self.batch_size = 450\n",
        "    # Dicionário com estatísticas básicas sobre os dados\n",
        "    # self.statistics = {\n",
        "    #     'n_exames': 0,\n",
        "    #     'n_reprov': 0,\n",
        "    #     'n_reprov_massa': 0,\n",
        "    #     'n_reprov_vol': 0,\n",
        "    #     'n_reprov_comp': 0,\n",
        "    #     'n_reprov_unid': 0,\n",
        "    # }\n",
        "    # Verificando se a pasta dos dados já existe. Se não existir, cria\n",
        "    if not os.path.isdir(DIR_PROJECT + '/analysis/' + self.dt):\n",
        "      os.mkdir(DIR_PROJECT + '/analysis/' + self.dt)\n",
        "\n",
        "\n",
        "  # df_name pode ser 'df_full', 'df_reprov', 'df_classif'\n",
        "  def __existsDataFrame(self, df_name):\n",
        "    match df_name:\n",
        "      case 'df_full':\n",
        "        try:\n",
        "          self.df_full = pd.read_csv(DIR_PROJECT + '/analysis/' + self.dt + '/full_data.csv')\n",
        "          return True\n",
        "        except FileNotFoundError:\n",
        "          return False\n",
        "      case 'df_reprov':\n",
        "        try:\n",
        "          self.df_reprov = pd.read_csv(DIR_PROJECT + '/analysis/' + self.dt + '/reprov_data.csv')\n",
        "          return True\n",
        "        except FileNotFoundError:\n",
        "          return False\n",
        "      case 'df_classif':\n",
        "        try:\n",
        "          self.df_classif = pd.read_csv(DIR_PROJECT + '/analysis/' + self.dt + '/classif_reprov_data.csv')\n",
        "          return True\n",
        "        except FileNotFoundError:\n",
        "          return False\n",
        "      case _:\n",
        "        return False\n",
        "\n",
        "  # Retorna DataFrame Pandas com todos os resultados\n",
        "  def miningDataInPDF(self):\n",
        "\n",
        "    if self.__existsDataFrame('df_full'):\n",
        "      return self.full_df\n",
        "    else:\n",
        "      # Extraindo páginas do PDF\n",
        "      pgs = pdfplumber.open(self.filename).pages\n",
        "\n",
        "      # Tabela temporária para usar no loop\n",
        "      tb = None\n",
        "\n",
        "      # DataFrame Pandas temporário para usar no loop com 12 colunas\n",
        "      df_temp = pd.DataFrame()\n",
        "\n",
        "      # DataFrame Pandas para todos os dados\n",
        "      df = pd.DataFrame()\n",
        "\n",
        "      # Lista com itens problemáticos encontrados na criação do DataFrame\n",
        "      list_problems = list()\n",
        "\n",
        "      list_data = list()\n",
        "\n",
        "      # Gerando o DataFrame Pandas para todos os dados\n",
        "      for pg in pgs:\n",
        "        tb = pg.extract_table()\n",
        "\n",
        "        list_data += tb[3:]\n",
        "\n",
        "        df_temp = pd.DataFrame(tb[3:])\n",
        "        df = pd.concat([df, df_temp])\n",
        "\n",
        "      # Definindo os rótulos do DataFrame Pandas final\n",
        "      rotulos = [\n",
        "          'laudo',\n",
        "          'data',\n",
        "          'resultado',\n",
        "          'i',\n",
        "          'm',\n",
        "          'im',\n",
        "          'produto',\n",
        "          'tc',\n",
        "          'marca',\n",
        "          'qn',\n",
        "          'quant',\n",
        "          'aleat'\n",
        "          ]\n",
        "\n",
        "      # Selecionando apenas as 12 colunas que interessam\n",
        "      df_dados_brutos = df.iloc[0:,0:12]\n",
        "      df_dados_brutos.columns = rotulos\n",
        "\n",
        "      # Eliminando linhas vazias e resetando os índices\n",
        "      df_dados_brutos = df_dados_brutos.dropna().reset_index()\n",
        "\n",
        "      # Salvando DataFrames na pasta do projeto como CSV\n",
        "      self.full_df = df_dados_brutos\n",
        "      self.reprov_df = self.full_df[self.full_df['resultado'] == 'Reprovado']\n",
        "\n",
        "      # Salvando CSVs com dados brutos e com reprovados\n",
        "      self.__makeDirAnalysis()\n",
        "      self.full_df.to_csv(DIR_PROJECT + '/analysis/' + self.dt + '/full_data.csv')\n",
        "      self.reprov_df.to_csv(DIR_PROJECT + '/analysis/' + self.dt + '/reprov_data.csv')\n",
        "\n",
        "      return df_dados_brutos\n",
        "\n",
        "\n",
        "  # Preenche o dicionário de estatísticas básicas obtidas sobre os dados brutos\n",
        "  # def __setStatisticsFromData(self, **kwargs):\n",
        "  #   df = self.df_dados_brutos\n",
        "\n",
        "  #   for key, value in kwargs.items():\n",
        "  #     self.statistics[key] = value\n",
        "\n",
        "\n",
        "  # Retorna lista com tuplas (laudo, produto) dado um DataFrame\n",
        "  def __getListaTuplas(self, df):\n",
        "    df_dados = df.iloc[0:, [1,7]]\n",
        "    lista_dic = df_dados.to_dict('records')\n",
        "\n",
        "    try:\n",
        "      spark_cont = SparkContext()\n",
        "    except Exception:\n",
        "      pass\n",
        "\n",
        "    rdd = spark_cont.parallelize(lista_dic)\n",
        "    lista = rdd.map(lambda item: (item['laudo'], item['produto'])).collect()\n",
        "\n",
        "    spark_cont.stop()\n",
        "\n",
        "    return lista\n",
        "\n",
        "\n",
        "  # Retorna DataFrame Pandas com todos os reprovados\n",
        "  def reprovToDataFrame(self):\n",
        "    if self.full_df is None:\n",
        "      self.miningDataInPDF()\n",
        "    df = self.full_df[self.full_df['resultado'] == 'Reprovado']\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "    self.reprov_df = df\n",
        "    return df\n",
        "\n",
        "  # Atualiza estatísticas básicas todas as vezes em que algum método para\n",
        "  # mineração de dados é usada\n",
        "  # def __updateBasicStatistics(self, **kwargs):\n",
        "  #   try:\n",
        "  #     self.statistics = {\n",
        "  #       'n_exames': kwargs.get('n_exames'),\n",
        "  #       'n_reprov': kwargs.get('n_reprov'),\n",
        "  #       'n_reprov_massa': kwargs.get('n_reprov_massa'),\n",
        "  #       'n_reprov_vol': kwargs.get('n_reprov_vol'),\n",
        "  #       'n_reprov_comp': kwargs.get('n_reprov_comp'),\n",
        "  #       'n_reprov_unid': kwargs.get('n_reprov_unid'),\n",
        "  #     }\n",
        "  #     return True\n",
        "  #   except Exception:\n",
        "  #     return False\n",
        "\n",
        "  # Cria diretório para armazenar arquivos de análise de dados se não existir\n",
        "  # Usar para salvar os DataFrames relevantes (PADRONIZAR NOMES)\n",
        "  def __makeDirAnalysis(self):\n",
        "    dt = self.dt\n",
        "    if not os.path.isdir(DIR_PROJECT + '/analysis/' + dt):\n",
        "      os.mkdir(DIR_PROJECT + '/analysis/' + dt)\n",
        "\n",
        "\n",
        "  # # Classifica os reprovados por segmento\n",
        "  # def classifyReprov(self):\n",
        "  #   if self.reprov_df is None:\n",
        "  #     self.reprovToDataFrame()\n",
        "\n",
        "\n",
        "  # Classifica os reprovados por segmento com IA\n",
        "  def classifyReprov(self):\n",
        "\n",
        "    if self.__existsDataFrame('df_classif'):\n",
        "      return self.reprov_classif_df\n",
        "    else:\n",
        "      # DataFrame com todos os reprovados para classificar\n",
        "      df = self.reprovToDataFrame()\n",
        "\n",
        "      # genai.configure(api_key=self.api_key)\n",
        "\n",
        "      ######################### Criação do modelo de IA #########################\n",
        "      generation_config = {\n",
        "        \"temperature\": 2,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 64,\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "      }\n",
        "\n",
        "      model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-1.5-pro\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=\"- Dada uma lista com dicionários Python com as chaves 'laudo' e 'produto', comparar os valores de 'produto' com os valores do dicionário abaixo para estimar o 'segmento' correspondente:\\n[\\n  {'segmento': 'Material de construção','produto': 'arame, argamassa, rejunte, separador de piso, abraçadeira, etc.'},\\n  {'segmento': 'Material elétrico', 'produto': 'fios, condutor elétrico, extensão elétrica, eletroduto, filtro de linha, fita isolante, passa-fio, etc.'},\\n  {'segmento': 'Material de escritório', 'produto': 'papel A4, fita adesiva, post-it, etc.'},\\n  {'segmento': 'Biscoitos e afins', 'produto': 'bolhachas, cookies, wafer, biscoito recheado, etc.'},\\n  {'segmento': 'Massas alimentícias','produto': 'macarrão, espaguete, lasanha, macarrões diversos, etc.'},\\n  {'segmento': 'Higiene pessoal','produto': 'álcool, álcool hidratado, creme dental, sabonete líquido, sabonete íntimo, algodão hidrófilo, etc.'},\\n  {'segmento': 'Material de limpeza','produto': 'água sanitária, desinfetante, alvejante, detergente, lava-louças, lava-roupas, sabão em pó'},\\n  {'segmento': 'Descartáveis para cozinha','produto': 'guardanapos, papel toalha, filtro de papel, papel alumínio, filme de PVC, etc.'},\\n  {'segmento': 'Descartáveis para festas','produto': 'sacolinhas para festas, papel para bombons, papel para bala, etc.'},\\n  {'segmento': 'Descartáveis para uso hospitalar','produto': 'abaixador de língua, lençol hospitalar, etc.'},\\n  {'segmento': 'Descartáveis para uso geral','produto': 'aqueles que não se enquadram nos outros descartáveis'},\\n  {'segmento': 'Utensílios para cozinha','produto': ''},\\n  {'segmento': 'produto têxtil','produto': 'babete, pano de chão, lençol, fronha de travesseiro, jogo de cama, pano de copa, tolha de rosto, toalha de banho, sianinha, cueiro, etc.'},\\n  {'segmento': 'Infláveis','produto': 'bóia inflável, bóia de braço, etc.'},\\n  {'segmento': 'Laticínios e achocolatados','produto': 'manteiga, creme de leite, queijos, leite achocolotado, composto lácteo (exceto os infatis), leite em pó, etc.'},\\n  {'segmento': 'Açúcar','produto': 'açúcar cristal, açúcar branco, açúcar demerara, açúcar para confeitar, etc.'},\\n  {'segmento': 'Farinhas','produto': 'farinha de mandioca, fTEXTILrnha de trigo, farinha de milho, flocão de milho, fubá, goma de tapioca, etc.'},\\n  {'segmento': 'Pescados','produto': 'filé de merluza (congelado ou não), atum (congelado ou não), peixe em posta (congelado ou não), camarão, sururu, etc.'},\\n  {'segmento': 'Adoçantes','produto': ''},\\n  {'segmento': 'Drenados','produto': 'azeitonas em conserva, dueto em conserva, sardnhas em óleo, atum em óleo, champingon em conserva, ervilhas em conserva, almôndegas, qualquer alimento em conserva ou ao molho, produtos com o termo PESO LÍQUIDO ou PESO DRENADO, etc.'},\\n  {'segmento': 'Condimentos e similares','produto': 'tempero, alho (em natura ou não), páprica, sal, caldo em tabletes, mostarda, catchup, molho para salada, etc.'},\\n  {'segmento': 'produtos cárneos','produto': 'mortadela, presunto, fiambre, salsicha, linguiça, etc.'},\\n  {'segmento': 'Alimento infantil','produto': 'farinha láctea, mingaus, cereal infantil, composto lácteo infantil, etc,'},\\n  {'segmento': 'Ração animal e similares','produto': 'ração para cães, ração para gatos, alpiste, etc.'},\\n  {'segmento': 'Grãos','produto': 'arroz, feijão, lentilha (exceto em conserva), milho (exceto em conserva) etc.'},\\n  {'segmento': 'Petiscos','produto': 'amendoim, amendoim japonês, batata frita, batata palha, etc.'},\\n  {'segmento': 'Bebidas não alcóolicas','produto': 'refrigerantes, sucos diversos, nectar de fruta, etc.'},\\n  {'segmento': 'Bebidas alcóolicas','produto': 'cerveja, chopp, vinho tinto, vinho seco, cachaça, aguardente, etc.'},\\n  {'segmento': 'Descartáveis para uso hospitalar','produto': 'abaixador de língua, lençol hospitalar, etc.'},\\n  {'segmento': 'Doces e afins','produto': 'goiabada, marmelada, doce de leite, doces de frutas, bombons, etc'},\\n  {'segmento': 'Café','produto': 'café em grão, café torrado e moído, café solúvel, etc.'},\\n  {'segmento': 'Cosméticos','produto': 'shanpoo, perfume, colônia, condicionador, creme de pentear, tintura para cabelos, etc.'},\\n  {'segmento': 'Diversos','produto': 'nenhum dos anteriores'},\\n]\\n- O resultado deve ser uma lista de tuplas com os valores das chaves 'laudo' e do segmento estimado do corresponde 'produto' associado ao 'laudo'\",\n",
        "      )\n",
        "\n",
        "      chat_session = model.start_chat(\n",
        "        history=[\n",
        "        ]\n",
        "      )\n",
        "      ######################### Fim da criação do modelo #########################\n",
        "\n",
        "\n",
        "      ################# Usando o modelo para estimar segmentos ###################\n",
        "\n",
        "      result_list = list() # Lista para os resultados em String\n",
        "      iteracoes = len(df) # Nº de iterações (registros do dataframe)\n",
        "      cont = self.batch_size # Tamanho do bloco de registros processados de uma vez\n",
        "      i_start = 0 # Índice de início do bloco\n",
        "      i_end = cont # Índice do fim do bloco\n",
        "      df_frac = None # DataFrame obtido a partir de cada bloco\n",
        "      df_final = None # DataFrame final com os resultados\n",
        "      lista_tuplas = None\n",
        "\n",
        "      # Tamanho do dataframe é igual ou menor que o tamanho do bloco\n",
        "      # Estimação dos segmentos é realizada de uma vez\n",
        "      if iteracoes <= cont:\n",
        "        lista_tuplas = self.__getListaTuplas(df)\n",
        "        response = chat_session.send_message(str(lista_tuplas))\n",
        "        result_list.append(response.text.split('\\n')[0])\n",
        "      else:\n",
        "        df_frac = df.iloc[i_start:i_end]\n",
        "        df_frac.reset_index(inplace=True, drop=True)\n",
        "        iteracoes -= cont\n",
        "        while iteracoes > 0:\n",
        "          lista_tuplas = self.__getListaTuplas(df_frac)\n",
        "          response = chat_session.send_message(str(lista_tuplas))\n",
        "          result_list.append(response.text.split('\\n')[0])\n",
        "          if iteracoes <= 500:\n",
        "            i_start = i_end\n",
        "            i_end = i_end + iteracoes\n",
        "            df_frac = df.iloc[i_start:i_end]\n",
        "            lista_tuplas = self.__getListaTuplas(df_frac)\n",
        "            response = chat_session.send_message(str(lista_tuplas))\n",
        "            result_list.append(response.text.split('\\n')[0])\n",
        "            break\n",
        "          else:\n",
        "            i_start += cont\n",
        "            i_end += cont\n",
        "            df_frac = df.iloc[i_start:i_end]\n",
        "            iteracoes -= cont\n",
        "      ################################## Fim #####################################\n",
        "\n",
        "      df_temp = pd.DataFrame()\n",
        "      df_final = None\n",
        "\n",
        "      for item in result_list:\n",
        "        df_temp = pd.concat([df_temp, pd.DataFrame(ast.literal_eval(item))])\n",
        "\n",
        "      df_temp.columns = ['laudo', 'segmento']\n",
        "      df_final = df.merge(df_temp, on='laudo')\n",
        "      df_final.reset_index(inplace=True, drop=True)\n",
        "\n",
        "      self.reprov_classif_df = df_final\n",
        "      self.reprov_classif_df.to_csv(DIR_PROJECT + '/analysis/' + self.dt + '/classif_reprov_data.csv')\n",
        "      # df_final.to_excel(DIR_PROJECT + '/classif_data.xlsx')\n",
        "      return df_final\n",
        "\n",
        "  # Constrói o gráfico de Pareto, dado um DataFrame 'df' com duas colunas\n",
        "  # sendo a primeira a variável 'causa' e a segunda as quantidades\n",
        "  def buildParetoGraph(self, df, col1, col2, label_x, label_y, title):\n",
        "    df_graf = df #df_classif.loc[:, ['segmento', 'quant']]\n",
        "    df_graf = df_graf.groupby(col1).count()\n",
        "\n",
        "    # Ordenando o DataFrame em ordem decrescente\n",
        "    df_graf_sorted = df_graf.sort_values(by=col2, ascending=False)\n",
        "\n",
        "    # Calculando a frequência acumulada\n",
        "    df_graf_sorted['cumperc'] = df_graf_sorted[col2].cumsum()/df_graf_sorted[col2].sum()*100\n",
        "\n",
        "    # Criando o gráfico de Pareto\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax1.bar(df_graf_sorted.index, df_graf_sorted[col2], color=\"C0\")\n",
        "    ax1.set_ylabel(label_y)\n",
        "    ax1.set_xticklabels(df_graf_sorted.index, rotation=90)\n",
        "\n",
        "    # Criando a linha da frequência acumulada\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(df_graf_sorted.index, df_graf_sorted['cumperc'], color=\"C1\", marker=\"D\", ms=7)\n",
        "    ax2.yaxis.set_major_formatter(PercentFormatter())\n",
        "    ax2.set_ylabel(label_y)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    df_tab = df.loc[:, [col1, col2]].groupby(col1).count()\n",
        "    df_tab.sort_values(by=col2, ascending=False, inplace=True)\n",
        "    df_tab.reset_index(inplace=True)\n",
        "    df_tab['freq_rel'] = df_tab[col2]*100/df_tab[col2].sum()\n",
        "    df_tab['freq_acum'] = df_tab['freq_rel'].cumsum()\n",
        "\n",
        "    lst = []\n",
        "    acum = 0\n",
        "    for item in df_tab.iterrows():\n",
        "      acum += item[1]['freq_rel']/100\n",
        "      if acum < 0.8:\n",
        "        lst.append([item[1][col1], '{:.1%}'.format(item[1]['freq_rel']/100), '{:1%}'.format(item[1]['freq_acum']/100)])\n",
        "      else:\n",
        "        break\n",
        "    lst.append([item[1][col1], '{:.1%}'.format(item[1]['freq_rel']/100), '{:1%}'.format(item[1]['freq_acum']/100)])\n",
        "    print(tb.tabulate(lst, headers=[col1.upper(), '%', '%Acum'], tablefmt='fancy_grid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEAznDKBlMtK"
      },
      "source": [
        "---\n",
        "---\n",
        "># **CARREGUE DADOS DE UMA DAS FORMAS ABAIXO**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "6d4dL0buLvxJ"
      },
      "outputs": [],
      "source": [
        "# @title => SE QUISER **USAR DADOS JÁ SALVOS**, EXECUTE ESTA CÉLULA <=\n",
        "import os.path\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "from ipywidgets import interact, widgets\n",
        "\n",
        "url_analysis = '/content/drive/MyDrive/Colab Notebooks/DataHandler/analysis'\n",
        "\n",
        "# Lista com nomes das pastas em .../DataHandler/analysis\n",
        "lista_itens = os.listdir(url_analysis)\n",
        "\n",
        "# Contador de opções do menu\n",
        "i = 1\n",
        "\n",
        "# Lista com os números das opções do menu para verificar se alguma foi selecionada\n",
        "list_opcoes = []\n",
        "# Lista com os itens de cada opção (Valor da opção, Data/Hora, Comentário)\n",
        "list_row = []\n",
        "# Lista com as linhas da tabela de opções\n",
        "list_msg = []\n",
        "\n",
        "try:\n",
        "  # Cabeçalho da tabela de opções...\n",
        "  cabec = ['OPÇÃO', 'DATA/HORA', 'COMENTÁRIO']\n",
        "\n",
        "  # Criando a lista de opções...\n",
        "  for item in lista_itens:\n",
        "    # Primeiro item: número da opção\n",
        "    list_row.append(str(i))\n",
        "\n",
        "    lst = item.split('|')\n",
        "    lstdt = lst[0].split('-')\n",
        "    dt = dtm.datetime(int(lstdt[0]), int(lstdt[1]), int(lstdt[2]))\n",
        "    data = dt.strftime('%d/%m/%Y')\n",
        "    hora = lst[1].split('.')\n",
        "\n",
        "    # Segundo item: Data/Hora\n",
        "    list_row.append(data + '/' + hora[0])\n",
        "\n",
        "    # Terceiro item: Comentário\n",
        "    f = open(url_analysis + '/' + item + '/coment.txt', 'r')\n",
        "    list_row.append(f.read())\n",
        "    f.close()\n",
        "\n",
        "    # Adicionando os valores da opção à lista c/ linhas das mensagens\n",
        "    list_msg.append(list_row)\n",
        "\n",
        "    # Resentando lista com itens de cada opção\n",
        "    list_row = []\n",
        "\n",
        "    list_opcoes.append(item)\n",
        "\n",
        "    i += 1\n",
        "  printMessage(list_msg, cabec)\n",
        "  print('\\nOBS. QUALQUER VALOR DIFERENTE ENCERRA ESTA TELA\\n')\n",
        "\n",
        "  opcao = str(input('DIGITE AQUI UMA DAS OPÇÕES ACIMA E PRESSIONE \"ENTER\" => '))\n",
        "\n",
        "# Usuário selecionou um conjunto de dados salvo\n",
        "  dh = DataHandler(dt=list_opcoes[int(opcao)-1])\n",
        "  df_full = dh.full_df\n",
        "  df_reprov = dh.reprov_df\n",
        "  df_classif = dh.reprov_classif_df\n",
        "  clear_output()\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('                     DADOS CARREGADOS COM SUCESSO')\n",
        "  print('------------------------------------------------------------------------\\n')\n",
        "  time.sleep(3)\n",
        "  clear_output()\n",
        "except KeyboardInterrupt:\n",
        "  clear_output()\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVJKtfnn6-94",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title => SE QUISER TRABALHAR COM **DADOS NOVOS**, EXECUTE ESTA CÉLULA <=\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "try:\n",
        "  clear_output()\n",
        "  print('------------------------------------------------------------------------------')\n",
        "  print('                     SELECIONE UM PDF NO SEU COMPUTADOR')\n",
        "  print('------------------------------------------------------------------------------\\n')\n",
        "  uploaded = files.upload()\n",
        "  filename = next(iter(uploaded))\n",
        "  coment = input('Digite um comentário sobre os dados e tecle ENTER (opcional):\\n')\n",
        "  print('------------------------------------------------------------------------------\\n')\n",
        "except KeyboardInterrupt:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  clear_output()\n",
        "  print('\\n----------------------------------------------------')\n",
        "  print('                     AGUARDE...')\n",
        "  print('----------------------------------------------------')\n",
        "  dh = DataHandler(filename=filename, coment=coment)\n",
        "  df_classif = dh.classifyReprov()\n",
        "  clear_output()\n",
        "  print('-----------------------------------------------------')\n",
        "  print('           PROCESSO CONCLUÍDO COM SUCESSO')\n",
        "  print('-----------------------------------------------------')\n",
        "  time.sleep(3)\n",
        "  clear_output()\n",
        "except KeyboardInterrupt:\n",
        "  clear_output()\n",
        "  pass\n",
        "except Exception as e:\n",
        "  clear_output()\n",
        "  title = 'Um dos seguintes erros ocorreu'\n",
        "  msg = ''\n",
        "  msg = '- O PDF enviado é muito grande para ser processado'\n",
        "  msg += '\\n- O PDF enviado não possui dados no formato esperado'\n",
        "  msg += '\\n- O arquivo enviado não é um PDF válido'\n",
        "  printMessage(msg, title)\n",
        "  try:\n",
        "    os.remove(filename)\n",
        "  except Exception:\n",
        "    clear_output()\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df = dh.full_df\n",
        "df.to_excel(DIR_PROJECT + '/data_test.xlsx')"
      ],
      "metadata": {
        "id": "ohLVe5zxoAcG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbAdhyN7ffZM"
      },
      "source": [
        "---\n",
        "---\n",
        "># **ESTATÍSTICAS**\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XAwYWP4F8HlM"
      },
      "outputs": [],
      "source": [
        "# @title ANÁLISE DE PARETO PARA OS SEGMENTOS DE PRÉ-EMBALADOS\n",
        "try:\n",
        "  df_graf1 = df_classif[df_classif['segmento'] != 'Diversos']\n",
        "  col1 = 'segmento'\n",
        "  col2 = 'quant'\n",
        "  dh.buildParetoGraph(df_graf1, 'segmento', 'quant', 'Segmento', 'Reprovações', 'Análise de Pareto para os segmentos reprovados')\n",
        "except Exception as e:\n",
        "  printMessage('Não há dados para exibir', 'ERRO!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q3KywGwY8uXU"
      },
      "outputs": [],
      "source": [
        "# @title COMPARATIVO ENTRE OS REPROVADOS POR TIPO DE CONTEÚDO NOMINAL (MASSA, VOLUME, N.º UNID. E COMPRIMENTO)\n",
        "try:\n",
        "  rp_massa = df_classif[df_classif['qn'].str.contains('g')]\n",
        "  rp_volume = df_classif[df_classif['qn'].str.contains('L')]\n",
        "  rp_unid_comp = df_classif.query('qn.str.contains(\"Un\") or qn.str.contains(\"m\")')\n",
        "\n",
        "  tot_mass = rp_massa.count()['quant']\n",
        "  tot_vol = rp_volume.count()['quant']\n",
        "  tot_unid_comp = rp_unid_comp.count()['quant']\n",
        "  tot_geral = tot_mass + tot_vol + tot_unid_comp\n",
        "\n",
        "  lst_vl = [tot_mass*100/tot_geral, tot_vol*100/tot_geral, tot_unid_comp*100/tot_geral]\n",
        "  df = pd.DataFrame(lst_vl, index=['Massa', 'Volume', 'Unid. e Comprimento'], columns=['Reprov'])\n",
        "  df.sort_values(by='Reprov', ascending=False, inplace=True)\n",
        "  ax = df.plot(kind='bar', legend=None, title='Percentual de reprovação por tipo de produto')\n",
        "  for i, v in enumerate(df['Reprov']):\n",
        "      ax.text(i, v, str(round(v, 2)) + '%', ha='center', va='bottom')\n",
        "except Exception:\n",
        "  printMessage('Não há dados para exibir', 'ERRO!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRNU8zz_89F9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ANÁLISE DE PARETO EM PRODUTOS COMERCIALIZADOS EM UNID. DE MASSA\n",
        "try:\n",
        "  df_graf2 = df_classif[df_classif['qn'].str.contains('g')]\n",
        "  col1 = 'segmento'\n",
        "  col2 = 'quant'\n",
        "  dh.buildParetoGraph(df_graf2, 'segmento', 'quant', 'Segmento', 'Reprovações', 'Análise de Pareto - Produtos em unid. de massa')\n",
        "except Exception:\n",
        "  printMessage('Não há dados para exibir', 'ERRO!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MenBCDScUqpb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ANÁLISE DE PARETO EM PRODUTOS COMERCIALIZADOS EM UNID. DE VOLUME\n",
        "try:\n",
        "  df_graf3 = df_classif[df_classif['qn'].str.contains('L')]\n",
        "  col1 = 'segmento'\n",
        "  col2 = 'quant'\n",
        "  dh.buildParetoGraph(df_graf3, 'segmento', 'quant', 'Segmento', 'Reprovações', 'Análise de Pareto - Produtos em unid. de volume')\n",
        "except Exception:\n",
        "  printMessage('Não há dados para exibir', 'ERRO!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ANÁLISE DE PARETO EM PRODUTOS COMERCIALIZADOS EM N.º DE UNID. E UNID. DE COMPRIMENTO\n",
        "try:\n",
        "  df_graf4 = df_classif.query('qn.str.contains(\"Un\") or qn.str.contains(\"m\")')\n",
        "  col1 = 'segmento'\n",
        "  col2 = 'quant'\n",
        "  dh.buildParetoGraph(df_graf4, 'segmento', 'quant', 'Segmento', 'Reprovações', 'Análise de Pareto - Produtos em N.º unid. e unid. de comprimento')\n",
        "except Exception:\n",
        "  printMessage('Não há dados para exibir', 'ERRO!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4RucvI19x5e0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}